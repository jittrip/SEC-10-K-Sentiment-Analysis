{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PT1a\n",
    "from hashlib import new\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "def write_page(url, file_path):\n",
    "    import requests\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',\n",
    "        }\n",
    "\n",
    "    r = requests.get(url, headers = headers)\n",
    "\n",
    "    filehandle = open(file_path, 'w')\n",
    "    filehandle.write(r.text)\n",
    "    filehandle.close()\n",
    "\n",
    "def download_file_10k(ticker, dest_folder):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(r'https://www.sec.gov/edgar/searchedgar/companysearch.html')\n",
    "    time.sleep(2)\n",
    "    click_search= r'//*[@id=\"company\"]'\n",
    "    driver.find_element('xpath', click_search).click()\n",
    "    driver.find_element('xpath', click_search).send_keys(ticker,Keys.ENTER)\n",
    "    ten_k=r'//*[@id=\"filingsStart\"]/div[2]/div[3]/h5'\n",
    "    time.sleep(2)\n",
    "    driver.find_element('xpath', ten_k).click()\n",
    "    view_all_10_k=r'//*[@id=\"filingsStart\"]/div[2]/div[3]/div/button[1]'\n",
    "    time.sleep(2)\n",
    "    driver.find_element('xpath', view_all_10_k).click()\n",
    "    time.sleep(2)\n",
    "    search_table=r'//*[@id=\"searchbox\"]'\n",
    "    driver.find_element('xpath', search_table).send_keys('10-K',Keys.ENTER)\n",
    "    time.sleep(3)\n",
    "    soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    results = [i.find('a')['href'] for i in soup.find_all('div',{'data-export':'Annual report [Section 13 and 15(d), not S-K Item 405]'})]\n",
    "    results = ['https://www.sec.gov' + i for i in results]\n",
    "    filing_date = [i for i in soup.find_all('td', {'class':'sorting_1'})]\n",
    "    filing_date_list=[]\n",
    "    for i in filing_date:\n",
    "        i=str(i)\n",
    "        filing_date_list.append(i[22:32])\n",
    "    time.sleep(2)\n",
    "    final_urls=[]\n",
    "    for i in results:\n",
    "        driver.get(i)\n",
    "        time.sleep(2)\n",
    "        dropdown=r'//*[@id=\"menu-dropdown-link\"]'\n",
    "        \n",
    "        try:\n",
    "            driver.find_element('xpath',dropdown).click()\n",
    "            time.sleep(.5)\n",
    "            soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            new_url_ending=[i['href'] for i in soup.find_all('a', {'id':'form-information-html'})]\n",
    "            new_url = ['https://www.sec.gov' + i for i in new_url_ending]\n",
    "            final_urls.append(new_url[0])\n",
    "        except:\n",
    "            final_urls.append(i)\n",
    "    for i,result in enumerate(final_urls):\n",
    "        write_page(result,f'{dest_folder}\\\\{ticker}_10-k_{filing_date_list[i]}.html')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PT2a)\n",
    "\n",
    "def clean_html_text(html_text: str) -> str:\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    ''' html_text : str\n",
    "        Raw html text read from a .html file\n",
    "    html_string : str\n",
    "        Contents pf html_text cleaned and stripped to leave just useful text \n",
    "        content\n",
    "    ''' \n",
    "    sec = BeautifulSoup(html_text,'html.parser') ## This is parsing the html text \n",
    "    html_string = sec.get_text(separator=' ')           # Returns all text elements of the html file.\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd=get_sp100()\n",
    "for i in asd[51:]:\n",
    "    download_file_10k(i, 'C:\\\\Users\\\\Kevin\\python\\\\download_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PT2b)\n",
    "def write_clean_html_text_files(input_folder, dest_folder):\n",
    "\n",
    "    from pathlib import Path\n",
    "    import os.path\n",
    "    import os\n",
    "    \n",
    "    # assign directory\n",
    "    \n",
    "    # iterate over files in\n",
    "    # that directory\n",
    "    #files = Path(input_folder).glob('*')\n",
    "    #for file in files:\n",
    "\n",
    "    for file in os.listdir(input_folder):\n",
    "\n",
    "        html_page = open(input_folder + '\\\\'+ file, \"r\")\n",
    "        clean_text = clean_html_text(html_page)\n",
    "\n",
    "        splitter = str(file).split('.')\n",
    "        filename = splitter[0]+'.txt'\n",
    "\n",
    "        completeName = os.path.join(dest_folder, filename)\n",
    "\n",
    "        filehandle = open(completeName, 'w', errors='ignore')\n",
    "        filehandle.write(clean_text)\n",
    "        filehandle.close()\n",
    "\n",
    "    return\n",
    "\n",
    "write_clean_html_text_files('C:\\\\Users\\\\Kevin\\python\\\\download_folder','C:\\\\Users\\\\Kevin\\\\python\\\\cleaned_files_folder')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JackWilkins\\Python\\Edgar Project\\Repo\\PodZinc\\Destinations\\AAPL_10-k_2012-10-31txt\n",
      "AAPL_10-k_2012-10-31txt\n"
     ]
    }
   ],
   "source": [
    "##PT2b notes\n",
    "import os.path\n",
    "dest_folder = 'C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Destinations'\n",
    "file = 'C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Downloads\\\\AAPL_10-k_2012-10-31.htm'\n",
    "filename = str(file)[-24:-4]+'txt'\n",
    "completeName = os.path.join(dest_folder, filename) \n",
    "print(completeName)\n",
    "print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART3a\n",
    "def get_sp100():\n",
    "\n",
    "    '''\n",
    "    Returns list of SP100 tickers as strings\n",
    "\n",
    "    PATH OF SP100 MUST BE UPDATED PER USER\n",
    "    HARD-CODED\n",
    "    '''\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    ticker_list = []\n",
    "\n",
    "    SP100 = pd.read_excel('C:\\\\Users\\\\Kevin\\\\python\\\\PodZinc-Part1-Production\\\\Production\\\\SP100.xlsx')\n",
    "\n",
    "    for x in SP100:\n",
    "\n",
    "        ticker_list = SP100['Symbol'].unique().tolist()\n",
    "    ticker_list.remove('BRK.B')\n",
    "    \n",
    "    return ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL',\n",
       " 'ABBV',\n",
       " 'ABT',\n",
       " 'ACN',\n",
       " 'ADBE',\n",
       " 'AIG',\n",
       " 'AMGN',\n",
       " 'AMT',\n",
       " 'AMZN',\n",
       " 'AVGO',\n",
       " 'AXP',\n",
       " 'BA',\n",
       " 'BAC',\n",
       " 'BK',\n",
       " 'BKNG',\n",
       " 'BLK',\n",
       " 'BMY',\n",
       " 'BRK.B',\n",
       " 'C',\n",
       " 'CAT',\n",
       " 'CHTR',\n",
       " 'CL',\n",
       " 'CMCSA',\n",
       " 'COF',\n",
       " 'COP',\n",
       " 'COST',\n",
       " 'CRM',\n",
       " 'CSCO',\n",
       " 'CVS',\n",
       " 'CVX',\n",
       " 'DD',\n",
       " 'DHR',\n",
       " 'DIS',\n",
       " 'DOW',\n",
       " 'DUK',\n",
       " 'EMR',\n",
       " 'EXC',\n",
       " 'F',\n",
       " 'FDX',\n",
       " 'GD',\n",
       " 'GE',\n",
       " 'GILD',\n",
       " 'GM',\n",
       " 'GOOG',\n",
       " 'GOOGL',\n",
       " 'GS',\n",
       " 'HD',\n",
       " 'HON',\n",
       " 'IBM',\n",
       " 'INTC',\n",
       " 'JNJ',\n",
       " 'JPM',\n",
       " 'KHC',\n",
       " 'KO',\n",
       " 'LIN',\n",
       " 'LLY',\n",
       " 'LMT',\n",
       " 'LOW',\n",
       " 'MA',\n",
       " 'MCD',\n",
       " 'MDLZ',\n",
       " 'MDT',\n",
       " 'MET',\n",
       " 'META',\n",
       " 'MMM',\n",
       " 'MO',\n",
       " 'MRK',\n",
       " 'MS',\n",
       " 'MSFT',\n",
       " 'NEE',\n",
       " 'NFLX',\n",
       " 'NKE',\n",
       " 'NVDA',\n",
       " 'ORCL',\n",
       " 'PEP',\n",
       " 'PFE',\n",
       " 'PG',\n",
       " 'PM',\n",
       " 'PYPL',\n",
       " 'QCOM',\n",
       " 'RTX',\n",
       " 'SBUX',\n",
       " 'SCHW',\n",
       " 'SO',\n",
       " 'SPG',\n",
       " 'T',\n",
       " 'TGT',\n",
       " 'TMO',\n",
       " 'TMUS',\n",
       " 'TSLA',\n",
       " 'TXN',\n",
       " 'UNH',\n",
       " 'UNP',\n",
       " 'UPS',\n",
       " 'USB',\n",
       " 'V',\n",
       " 'VZ',\n",
       " 'WBA',\n",
       " 'WFC',\n",
       " 'WMT',\n",
       " 'XOM']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sp100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART3b\n",
    "def get_yahoo_data(start_date, end_date, tickers):\n",
    "\n",
    "    import pandas as pd\n",
    "    import yfinance as yf\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for i, ticker in enumerate(tickers):\n",
    "        df_yahoo = yf.download(tickers[i],\n",
    "        start= start_date,\n",
    "        end= end_date,\n",
    "        progress=False,\n",
    "        group_by = tickers)\n",
    "        df = pd.DataFrame(df_yahoo)\n",
    "\n",
    "        df = df.rename(columns={'Adj Close': 'Price'})  ##df.pop to remove columns\n",
    "        df = df.reindex(columns=['High', 'Low','Price', 'Volume'])\n",
    "        df['1daily_returns'] = df['Price'].pct_change(1)\n",
    "        df['2daily_returns'] = df['Price'].pct_change(2)\n",
    "        df['3daily_returns'] = df['Price'].pct_change(3)\n",
    "        df['5daily_returns'] = df['Price'].pct_change(5)\n",
    "        df['10daily_returns'] = df['Price'].pct_change(10)\n",
    "        df['Symbol'] = tickers[i]\n",
    "\n",
    "        #df = df.fillna(0)\n",
    "\n",
    "        final_df = pd.concat([final_df, df], axis=0)\n",
    "\n",
    "    return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume</th>\n",
       "      <th>1daily_returns</th>\n",
       "      <th>2daily_returns</th>\n",
       "      <th>3daily_returns</th>\n",
       "      <th>5daily_returns</th>\n",
       "      <th>10daily_returns</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>2.830000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>14704800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-03</th>\n",
       "      <td>2.900000</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>11733600</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-04</th>\n",
       "      <td>2.910000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>12524300</td>\n",
       "      <td>-0.013937</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-05</th>\n",
       "      <td>2.870000</td>\n",
       "      <td>2.770000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>9635100</td>\n",
       "      <td>-0.010601</td>\n",
       "      <td>-0.024390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-06</th>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.760000</td>\n",
       "      <td>8932900</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>-0.024735</td>\n",
       "      <td>-0.038327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-07</th>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.760000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>7293800</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.017668</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>27.010000</td>\n",
       "      <td>26.802500</td>\n",
       "      <td>24.272789</td>\n",
       "      <td>178557200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-03</th>\n",
       "      <td>27.575001</td>\n",
       "      <td>27.002501</td>\n",
       "      <td>24.587440</td>\n",
       "      <td>209130400</td>\n",
       "      <td>0.012963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-04</th>\n",
       "      <td>27.372499</td>\n",
       "      <td>26.930000</td>\n",
       "      <td>24.407635</td>\n",
       "      <td>166297600</td>\n",
       "      <td>-0.007313</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-05</th>\n",
       "      <td>27.325001</td>\n",
       "      <td>27.032499</td>\n",
       "      <td>24.466072</td>\n",
       "      <td>149743600</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>-0.004936</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-06</th>\n",
       "      <td>27.197500</td>\n",
       "      <td>26.950001</td>\n",
       "      <td>24.536045</td>\n",
       "      <td>139874000</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>-0.002090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-07</th>\n",
       "      <td>27.330000</td>\n",
       "      <td>27.137501</td>\n",
       "      <td>24.606020</td>\n",
       "      <td>134766000</td>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 High        Low      Price     Volume  1daily_returns  \\\n",
       "Date                                                                     \n",
       "2014-10-31   2.830000   2.750000   2.800000   14704800             NaN   \n",
       "2014-11-03   2.900000   2.810000   2.870000   11733600        0.025000   \n",
       "2014-11-04   2.910000   2.750000   2.830000   12524300       -0.013937   \n",
       "2014-11-05   2.870000   2.770000   2.800000    9635100       -0.010601   \n",
       "2014-11-06   2.800000   2.750000   2.760000    8932900       -0.014286   \n",
       "2014-11-07   2.800000   2.760000   2.780000    7293800        0.007246   \n",
       "2014-10-31  27.010000  26.802500  24.272789  178557200             NaN   \n",
       "2014-11-03  27.575001  27.002501  24.587440  209130400        0.012963   \n",
       "2014-11-04  27.372499  26.930000  24.407635  166297600       -0.007313   \n",
       "2014-11-05  27.325001  27.032499  24.466072  149743600        0.002394   \n",
       "2014-11-06  27.197500  26.950001  24.536045  139874000        0.002860   \n",
       "2014-11-07  27.330000  27.137501  24.606020  134766000        0.002852   \n",
       "\n",
       "            2daily_returns  3daily_returns  5daily_returns  10daily_returns  \\\n",
       "Date                                                                          \n",
       "2014-10-31             NaN             NaN             NaN              NaN   \n",
       "2014-11-03             NaN             NaN             NaN              NaN   \n",
       "2014-11-04        0.010714             NaN             NaN              NaN   \n",
       "2014-11-05       -0.024390        0.000000             NaN              NaN   \n",
       "2014-11-06       -0.024735       -0.038327             NaN              NaN   \n",
       "2014-11-07       -0.007143       -0.017668       -0.007143              NaN   \n",
       "2014-10-31             NaN             NaN             NaN              NaN   \n",
       "2014-11-03             NaN             NaN             NaN              NaN   \n",
       "2014-11-04        0.005555             NaN             NaN              NaN   \n",
       "2014-11-05       -0.004936        0.007963             NaN              NaN   \n",
       "2014-11-06        0.005261       -0.002090             NaN              NaN   \n",
       "2014-11-07        0.005720        0.008128        0.013729              NaN   \n",
       "\n",
       "           Symbol  \n",
       "Date               \n",
       "2014-10-31    AMD  \n",
       "2014-11-03    AMD  \n",
       "2014-11-04    AMD  \n",
       "2014-11-05    AMD  \n",
       "2014-11-06    AMD  \n",
       "2014-11-07    AMD  \n",
       "2014-10-31   AAPL  \n",
       "2014-11-03   AAPL  \n",
       "2014-11-04   AAPL  \n",
       "2014-11-05   AAPL  \n",
       "2014-11-06   AAPL  \n",
       "2014-11-07   AAPL  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_yahoo_data('2014-10-31','2014-11-09', ['AMD','AAPL'])#.to_csv('C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Destinations', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1\n",
    "import edgar_downloader\n",
    "edgar_downloader.download_file_10k('AMD', 'C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Downloads')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Downloads'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kevin\\python\\PodZinc-Part1-Production\\Production\\Parts1-3 notebook.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m#PART 2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39medgar_cleaner\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000010?line=2'>3</a>\u001b[0m edgar_cleaner\u001b[39m.\u001b[39;49mwrite_clean_html_text_files(\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mJackWilkins\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mPython\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mEdgar Project\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mRepo\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mPodZinc\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDownloads\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mJackWilkins\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mPython\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mEdgar Project\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mRepo\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mPodZinc\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDestinations\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\python\\PodZinc-Part1-Production\\Production\\edgar_cleaner.py:27\u001b[0m, in \u001b[0;36mwrite_clean_html_text_files\u001b[1;34m(input_folder, dest_folder)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(input_folder):\n\u001b[0;32m     29\u001b[0m     html_page \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(input_folder \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m file, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m     clean_text \u001b[39m=\u001b[39m clean_html_text(html_page)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Downloads'"
     ]
    }
   ],
   "source": [
    "#PART 2\n",
    "import edgar_cleaner\n",
    "edgar_cleaner.write_clean_html_text_files('C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Downloads','C:\\\\Users\\\\JackWilkins\\\\Python\\\\Edgar Project\\\\Repo\\\\PodZinc\\\\Destinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AIG', 'AMGN', 'AMT', 'AMZN', 'AVGO', 'AXP', 'BA', 'BAC', 'BK', 'BKNG', 'BLK', 'BMY', 'BRK.B', 'C', 'CAT', 'CHTR', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', 'DD', 'DHR', 'DIS', 'DOW', 'DUK', 'EMR', 'EXC', 'F', 'FDX', 'GD', 'GE', 'GILD', 'GM', 'GOOG', 'GOOGL', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KHC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'MA', 'MCD', 'MDLZ', 'MDT', 'MET', 'META', 'MMM', 'MO', 'MRK', 'MS', 'MSFT', 'NEE', 'NFLX', 'NKE', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PM', 'PYPL', 'QCOM', 'RTX', 'SBUX', 'SCHW', 'SO', 'SPG', 'T', 'TGT', 'TMO', 'TMUS', 'TSLA', 'TXN', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VZ', 'WBA', 'WFC', 'WMT', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "#PART 3a\n",
    "import ref_data as edgar_data\n",
    "tickers_sp100= edgar_data.get_sp100()\n",
    "print(tickers_sp100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "- BRK.B: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "#PART 3b\n",
    "import ref_data as edgar_data\n",
    "#df_returns = edgar_data.get_yahoo_data('2000-01-01', '2020-08-01', edgar_data.get_sp100())\n",
    "#display(df_returns)\n",
    "\n",
    "edgar_data.get_yahoo_data('2020-12-12','2021-12-12', edgar_data.get_sp100()).to_csv('C:\\\\Users\\\\Kevin\\\\python\\\\PodZinc-Part1-Production\\\\Production\\\\Get_Yahoo_Finance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 3c\n",
    "import ref_data as edgar_data\n",
    "sentiment_dict= edgar_data.get_sentiment_word_dict()\n",
    "print(sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b(?i)negative\\b|\\b(?i)positive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b\n",
      "['Uncertainty', 'negative', 'uncertainty', 'uncertainty', 'uncertainty', 'uncertainty', 'uncertainty', 'uncertainty', 'positive', 'positive']\n",
      "\\b(?i)negative\\b|\\b(?i)positive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b\n",
      "['Uncertainty', 'negative', 'uncertainty', 'uncertainty', 'negative', 'uncertainty', 'positive', 'positive', 'uncertainty']\n",
      "\\b(?i)negative\\b|\\b(?i)positive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b\n",
      "['Uncertainty', 'negative', 'uncertainty', 'uncertainty', 'negative', 'uncertainty', 'positive', 'positive', 'uncertainty']\n",
      "\\b(?i)negative\\b|\\b(?i)positive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b\n",
      "['Uncertainty', 'negative', 'uncertainty', 'uncertainty', 'negative', 'uncertainty', 'positive', 'positive', 'positive', 'uncertainty', 'positive']\n",
      "\\b(?i)negative\\b|\\b(?i)positive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def write_document_sentiments(input_folder,output_file):\n",
    "\n",
    "    import os\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    \n",
    "\n",
    "\n",
    "    dfbase=None\n",
    "    dfadd_on=None\n",
    "    for filename in os.listdir(input_folder):\n",
    "        full_path = os.path.join(input_folder,filename)\n",
    "        cleaned_file=open(full_path,'r')\n",
    "        cleaned_txt=cleaned_file.read()\n",
    "        file_split=str(filename).split('_')\n",
    "        #print(file_split)\n",
    "        #print(filename)\n",
    "        #keywords=re.findall(r'\\baaa\\b|\\bbbb\\b', cleaned_txt,re.DOTALL)\n",
    "        o=r'\\b(?i)negative\\b|\\b(?i)positive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b'\n",
    "        print(o)\n",
    "        keywords=re.findall(o, cleaned_txt,re.DOTALL)\n",
    "        print(keywords)\n",
    "        \n",
    "        # negative=re.findall('aaa', cleaned_txt,re.DOTALL)\n",
    "        # positive=re.findall('aaa', cleaned_txt,re.DOTALL)\n",
    "        # uncertainty=re.findall('aaa', cleaned_txt,re.DOTALL)\n",
    "        # constraining=re.findall('aaa', cleaned_txt,re.DOTALL)\n",
    "        # superfluous=re.findall('aaa', cleaned_txt,re.DOTALL)\n",
    "        # interesting=re.findall('aaa', cleaned_txt,re.DOTALL)\n",
    "        # modal=re.findall('aaa', cleaned_txt,re.DOTALL)\n",
    "        #print(negative)\n",
    "        #print(keywords)\n",
    "        # aaa_count=keywords.count('aaa')\n",
    "        # bbb_count=keywords.count('bbb')\n",
    "    # #     symbol=file_split[0]\n",
    "    # #     reporttype=file_split[1]\n",
    "    # #     filingdate=file_split[2][:-4]\n",
    "    # #     negative_count=keywords.count('negative')\n",
    "    # #     positive_count=keywords.count('positive')\n",
    "    # #     uncertainty_count=keywords.count('uncertainty')\n",
    "    # #     litigious_count=keywords.count('litigious')\n",
    "    # #     constraining_count=keywords.count('constraining')\n",
    "    # #     superfluous_count=keywords.count('superfluous')\n",
    "    # #     interesting_count=keywords.count('interesting')\n",
    "    # #     modal_count=keywords.count('modal')\n",
    "        \n",
    "    # #     #print(aaa_count)\n",
    "    # #     #print(bbb_count)\n",
    "    # #     #df=pd.DataFrame([[aaa_count, bbb_count]], columns=['aaa','bbb'])\n",
    "    # #     df=pd.DataFrame([[symbol,reporttype,filingdate,negative_count, positive_count,uncertainty_count, litigious_count, constraining_count, superfluous_count,interesting_count, modal_count]], columns=['Symbol','ReportType','FilingDate','Negative','Positive', 'Uncertainty','Litigious','Constraining','Superfluous', 'Interesting', 'Modal'])\n",
    "    # #     #print(df)\n",
    "    # #     dfadd_on=df\n",
    "    # #     base=pd.concat([dfbase,dfadd_on], axis=0)\n",
    "    # #     #print(base)\n",
    "    # #     dfbase=base\n",
    "    # # dfbase.to_csv(output_file, index=False)\n",
    "\n",
    "write_document_sentiments('C:\\\\Users\\\\Kevin\\\\python\\\\cleaned_files_folder', 'C:\\\\Users\\\\Kevin\\\\python\\\\export_csv_folder\\\\sentiment_factors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 3209: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kevin\\python\\PodZinc-Part1-Production\\Production\\Parts1-3 notebook.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=51'>52</a>\u001b[0m     dfbase\u001b[39m.\u001b[39mto_csv(output_file, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=52'>53</a>\u001b[0m     \u001b[39m# for i in variants_list:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=53'>54</a>\u001b[0m     \u001b[39m#     sentiment_str = sentiment_str +rf'\\b(?i){i}\\b|'\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=54'>55</a>\u001b[0m     \u001b[39m#     sentiment_str_fin=(sentiment_str[:-1])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=64'>65</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=65'>66</a>\u001b[0m \u001b[39m#keywords=re.findall(r'\\bnegative\\b|\\bpositive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b', cleaned_txt,re.DOTALL)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=67'>68</a>\u001b[0m write_document_sentiments (\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mKevin\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mpython\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mcleaned_files_folder\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mKevin\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mpython\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mexport_csv_folder\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39msentiment_factors.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Kevin\\python\\PodZinc-Part1-Production\\Production\\Parts1-3 notebook.ipynb Cell 16\u001b[0m in \u001b[0;36mwrite_document_sentiments\u001b[1;34m(input_folder, output_file)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=16'>17</a>\u001b[0m full_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(input_folder,filename)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=17'>18</a>\u001b[0m cleaned_file\u001b[39m=\u001b[39m\u001b[39mopen\u001b[39m(full_path,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=18'>19</a>\u001b[0m cleaned_txt\u001b[39m=\u001b[39mcleaned_file\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=19'>20</a>\u001b[0m file_split\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(filename)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/python/PodZinc-Part1-Production/Production/Parts1-3%20notebook.ipynb#ch0000015?line=20'>21</a>\u001b[0m symbol\u001b[39m=\u001b[39mfile_split[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 3209: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "def write_document_sentiments(input_folder,output_file):\n",
    "    import ref_data as edgar_data\n",
    "    import os\n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "    dfbase=None\n",
    "    dfadd_on=None\n",
    "    sentiment_dict= edgar_data.get_sentiment_word_dict()\n",
    "    variants_list=None\n",
    "    sentiment_str=''\n",
    "    occurances=[]\n",
    "\n",
    "    \n",
    "        #print(variants_list)\n",
    "    for filename in os.listdir(input_folder):\n",
    "        full_path = os.path.join(input_folder,filename)\n",
    "        cleaned_file=open(full_path,'r')\n",
    "        cleaned_txt=cleaned_file.read()\n",
    "        file_split=str(filename).split('_')\n",
    "        symbol=file_split[0]\n",
    "        reporttype=file_split[1]\n",
    "        filingdate=file_split[2][:-4]\n",
    "        occurances.append(symbol)\n",
    "        occurances.append(reporttype)\n",
    "        occurances.append(filingdate)\n",
    "        for lists in sentiment_dict:\n",
    "            variants_list = sentiment_dict[lists]\n",
    "            #print(lists)\n",
    "            for words in variants_list:\n",
    "                #print(words)\n",
    "                sentiment_str = sentiment_str+f'\\\\b(?i){words}\\\\b|'\n",
    "            sentiment_str_fin=(sentiment_str[:-1])\n",
    "            sentiment_str=''\n",
    "        #print(sentiment_str_fin)\n",
    "    #print(sentiment_str_fin)\n",
    "            keywords=re.findall(sentiment_str_fin,cleaned_txt, re.DOTALL)\n",
    "            length=len(keywords)\n",
    "            keywords=''\n",
    "            occurances.append(length)\n",
    "        df=pd.DataFrame([occurances], columns=['Symbol','ReportType','FilingDate','Negative','Positive','Uncertainty','Litigious','Strong_Modal','Weak_Modal', 'Constraining'])\n",
    "        #print(df)\n",
    "        dfadd_on=df\n",
    "        base=pd.concat([dfbase,dfadd_on], axis=0)\n",
    "        #print(base)\n",
    "        dfbase=base\n",
    "        occurances=[]\n",
    "        # print(dfbase)\n",
    "        #print(aggregate_occurance)\n",
    "        # print(keywords)\n",
    "        # print(variants_list)\n",
    "    dfbase.to_csv(output_file, index=False)\n",
    "    # for i in variants_list:\n",
    "    #     sentiment_str = sentiment_str +rf'\\b(?i){i}\\b|'\n",
    "    #     sentiment_str_fin=(sentiment_str[:-1])\n",
    "    #     keywords=re.findall(sentiment_str_fin,cleaned_txt, re.DOTALL)\n",
    "    #     #print(keywords)\n",
    "    # print(sentiment_str_fin)\n",
    "        #Take off final | from string\n",
    "\n",
    "        #when word exists in doc\n",
    "        #add one to counter\n",
    "        #append each word counter to sentiment counter\n",
    "        #put sentiment counter as column in csv\n",
    "\n",
    "#keywords=re.findall(r'\\bnegative\\b|\\bpositive\\b|\\buncertainty\\b|\\blitigious\\b|\\bconstraining\\b|\\bsuperfluous\\b|\\bmodal\\b', cleaned_txt,re.DOTALL)\n",
    "\n",
    "write_document_sentiments ('C:\\\\Users\\\\Kevin\\\\python\\\\cleaned_files_folder', 'C:\\\\Users\\\\Kevin\\\\python\\\\export_csv_folder\\\\sentiment_factors.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2553770668.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [4]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(0)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import ref_data as edgar_data\n",
    "\n",
    "def joined_data(document_sentiments_input, output_file, ticker):\n",
    "    dfbase=None\n",
    "    dfadd_on=None\n",
    "    df=pd.read_csv(document_sentiments_input)\n",
    "    for index,a in enumerate(ticker):\n",
    "        df2=df[(df == a).any(axis=1)]\n",
    "        filing_date_list=(df2['FilingDate'].to_list())\n",
    "        company=ticker[index]\n",
    "        company_in_list=[]\n",
    "        company_in_list.append(company)\n",
    "        print(0)\n",
    "        for i, date in enumerate(filing_date_list):\n",
    "            ending_date=datetime.strptime(date,\"%Y-%m-%d\")+timedelta(days=20)\n",
    "            ending_date_str=str(ending_date)\n",
    "            split=ending_date_str.split()\n",
    "            end_date=split[0]\n",
    "            # print(date)\n",
    "            # print(end_date)\n",
    "            #print(company)\n",
    "            # print(date)\n",
    "            # print(end_date)\n",
    "            # print(company[0])\n",
    "            yahoo_data=edgar_dataget_yahoo_data(date,end_date, company_in_list)\n",
    "            resetted_yahoo_data=yahoo_data.reset_index()\n",
    "            #display(resetted_yahoo_data)\n",
    "            #display(yahoo_data)\n",
    "            words_row=df2.iloc[i]\n",
    "            #print(words_row)\n",
    "            worddf=pd.DataFrame(words_row)\n",
    "            transposed=worddf.transpose()\n",
    "            #display(transposed)\n",
    "            concatteddf=pd.merge(resetted_yahoo_data, transposed, on = 'Symbol', how='left')\n",
    "            #display(concatteddf)\n",
    "            dfbase=pd.concat([dfbase,concatteddf], axis=0)\n",
    "    #display(dfbase)\n",
    "    dfbase.to_csv(output_file, index=False)\n",
    "\n",
    "joined_data('C:\\\\Users\\\\Kevin\\\\python\\\\export_csv_folder\\\\sentiment_factors_demo.csv','C:\\\\Users\\\\Kevin\\\\python\\\\final_export_csv_folder\\\\final_sentiment_demo.csv' ,['AAPL','ABT','META'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f4f70181dd95a9d562a70f836fcadba96ae89ae4a502bd83f1e00f915c26e41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
